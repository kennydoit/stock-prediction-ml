{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366d0185",
   "metadata": {},
   "source": [
    "# Feature Engineering for Stock Prediction\n",
    "\n",
    "This notebook handles feature engineering and analysis for stock price prediction, including technical indicators and correlation analysis across target and peer assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d365d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 09:29:30,632 - INFO - Imports and configurations loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('../')\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "# Local imports from src\n",
    "from src.data_loader import StockDataLoader\n",
    "from src.features import FeatureEngineer\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "logger.info(\"Imports and configurations loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03594f67",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1e27b1",
   "metadata": {},
   "source": [
    "## Import and create stock features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c9191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 09:29:30,693 - INFO - Target Symbol: MSFT\n",
      "2025-06-01 09:29:30,695 - INFO - Filtered Peer Symbols: AAPL, AMZN, GOOGL, META, ORCL, IBM, CRM, ADBE, NVDA, INTC, SPY, QQQ, XLK, VTI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features saved to '../data/processed/all_features.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Add logging configuration\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load configuration\n",
    "with open('../config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Initialize components\n",
    "data_loader = StockDataLoader('../config.yaml')\n",
    "feature_engineer = FeatureEngineer(config)  # Pass config to FeatureEngineer\n",
    "\n",
    "# Set date range\n",
    "end_date = datetime.now()\n",
    "start_date = datetime(2024, 1, 1)  # Start from beginning of 2024\n",
    "\n",
    "# Get symbols from config\n",
    "# Get symbols from config and filter out target from peers\n",
    "target_symbol = config['target_symbol']\n",
    "peer_symbols = [symbol for symbol in config['peer_symbols'] if symbol != target_symbol]\n",
    "\n",
    "# Log the filtered symbols\n",
    "logger.info(f\"Target Symbol: {target_symbol}\")\n",
    "logger.info(f\"Filtered Peer Symbols: {', '.join(peer_symbols)}\")\n",
    "\n",
    "# Load target asset data\n",
    "target_asset = data_loader.fetch_stock_data(target_symbol, \n",
    "                                          start_date.strftime('%Y-%m-%d'),\n",
    "                                          end_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "# Load peer assets data\n",
    "peer_assets = {}\n",
    "for symbol in peer_symbols:\n",
    "    peer_assets[symbol] = data_loader.fetch_stock_data(symbol,\n",
    "                                                      start_date.strftime('%Y-%m-%d'),\n",
    "                                                      end_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "# Calculate features for target asset\n",
    "target_features = feature_engineer.engineer_features(target_asset)\n",
    "target_features.columns = [f'{target_symbol}_{col}' for col in target_features.columns]\n",
    "\n",
    "# Calculate and combine features for peer assets\n",
    "peer_features_list = []\n",
    "for symbol, data in peer_assets.items():\n",
    "    features = feature_engineer.engineer_features(data)\n",
    "    features.columns = [f'{symbol}_{col}' for col in features.columns]\n",
    "    peer_features_list.append(features)\n",
    "\n",
    "# Combine all peer features into a single DataFrame\n",
    "\n",
    "peer_features = pd.concat(peer_features_list, axis=1)\n",
    "\n",
    "# Display first few rows of features\n",
    "target_features.head()\n",
    "peer_features.head()\n",
    "\n",
    "# Combine target and peer features\n",
    "all_features = pd.concat([target_features, peer_features], axis=1)\n",
    "\n",
    "# Display first few rows of combined features\n",
    "all_features.head()\n",
    "\n",
    "# Create data/processed directory if it doesn't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save features to pickle file\n",
    "all_features.to_pickle('../data/processed/all_features.pkl')\n",
    "print(\"\\nFeatures saved to '../data/processed/all_features.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd19524",
   "metadata": {},
   "source": [
    "## Import and create sentiment features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66bcd53",
   "metadata": {},
   "source": [
    "## Note on Sentiment Analysis\n",
    "Sentiment analysis has been temporarily removed due to API limitations:\n",
    "- Free tier only provides 30 days of historical data\n",
    "- Paid API access is cost-prohibitive for this project\n",
    "- Focus will be on technical indicators for initial model development\n",
    "\n",
    "Future enhancements may include:\n",
    "- Alternative news data sources\n",
    "- Social media sentiment analysis\n",
    "- Web scraping implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e45b4b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 09:29:40,142 - INFO - Initialized SentimentAnalyzer with endpoint: https://newsapi.org/v2\n",
      "2025-06-01 09:29:40,143 - INFO - Sentiment analyzer initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Load configuration and initialize sentiment analyzer with validation\n",
    "try:\n",
    "    with open('../config.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    # Verify sentiment configuration exists\n",
    "    if 'sentiment' not in config:\n",
    "        raise ValueError(\"Missing 'sentiment' section in config.yaml\")\n",
    "    \n",
    "    # Verify API key exists and is not empty\n",
    "    if not config['sentiment'].get('api_key'):\n",
    "        raise ValueError(\"Missing or empty API key in sentiment configuration\")\n",
    "    \n",
    "    # Initialize sentiment analyzer with verified config\n",
    "    from src.features.sentiment.analyzer import SentimentAnalyzer\n",
    "    sentiment_analyzer = SentimentAnalyzer(config)\n",
    "    \n",
    "    logger.info(\"Sentiment analyzer initialized successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize sentiment analyzer: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c39ba54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching sentiment data for MSFT from 2025-05-03 to 2025-06-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 09:34:13,758 - ERROR - Error fetching news data: 426 Client Error: Upgrade Required for url: https://newsapi.org/v2/everything?q=%22MSFT%22+AND+%28stock+OR+market+OR+trading%29&from=2025-05-03&to=2025-06-01&language=en&sortBy=publishedAt&page=2&pageSize=100&apiKey=7b5607dfb91a4cce947e77392bd7b58f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features retrieved:\n",
      "sentiment_score: 0.6068159999999999\n",
      "sentiment_magnitude: 0.17207999999999998\n",
      "article_count: 100\n",
      "Total articles: 100\n",
      "\n",
      "Sample articles:\n",
      "- 2025-05-31T11:01:03Z: Top Technology Stocks To Follow Today â€“ May 29th\n",
      "- 2025-05-31T11:00:00Z: Nvidia can't be stopped, Apple falls behind, and the AI data center race: Tech news roundup\n",
      "- 2025-05-31T08:46:01Z: BI Investor of the Month: Meet the fund manager up 28% over the last year\n"
     ]
    }
   ],
   "source": [
    "# Use historical dates within NewsAPI's 30-day limit\n",
    "end_date = datetime.now() \n",
    "start_date = end_date - timedelta(days=29)     # Last 29 days (total 30 days with end date)\n",
    "\n",
    "print(f\"\\nFetching sentiment data for MSFT from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "try:\n",
    "    features = sentiment_analyzer.get_sentiment_features('MSFT', end_date, start_date)\n",
    "    \n",
    "    # Debug output\n",
    "    print(\"\\nFeatures retrieved:\")\n",
    "    for key, value in features.items():\n",
    "        if key != 'articles':  # Skip printing full article list\n",
    "            print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(f\"Total articles: {len(value)}\")\n",
    "            if len(value) > 0:\n",
    "                print(\"\\nSample articles:\")\n",
    "                for article in value[:3]:\n",
    "                    print(f\"- {article['publishedAt']}: {article['title']}\")\n",
    "                    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error fetching sentiment data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentiment_dates(symbol, start_date, end_date):\n",
    "    \"\"\"Test sentiment retrieval for a date range\"\"\"\n",
    "    print(f\"\\nTesting sentiment for {symbol} from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    try:\n",
    "        # Get raw sentiment data\n",
    "        raw_data = sentiment_analyzer.get_raw_sentiment(symbol)\n",
    "        \n",
    "        if raw_data and 'articles' in raw_data:\n",
    "            # Filter articles by date range after retrieval\n",
    "            articles = raw_data['articles']\n",
    "            filtered_articles = [\n",
    "                article for article in articles\n",
    "                if start_date.strftime('%Y-%m-%d') <= article.get('publishedAt', '').split('T')[0] <= end_date.strftime('%Y-%m-%d')\n",
    "            ]\n",
    "            \n",
    "            # Process filtered articles\n",
    "            if filtered_articles:\n",
    "                dates = [article.get('publishedAt', '').split('T')[0] for article in filtered_articles]\n",
    "                \n",
    "                print(f\"\\nArticle date distribution:\")\n",
    "                date_counts = pd.Series(dates).value_counts().sort_index()\n",
    "                print(date_counts)\n",
    "                \n",
    "                print(f\"\\nDate range summary:\")\n",
    "                print(f\"Total articles retrieved: {len(articles)}\")\n",
    "                print(f\"Articles in date range: {len(filtered_articles)}\")\n",
    "                print(f\"Earliest article: {min(dates) if dates else 'No articles'}\")\n",
    "                print(f\"Latest article: {max(dates) if dates else 'No articles'}\")\n",
    "                \n",
    "                # Sample some articles\n",
    "                if filtered_articles:\n",
    "                    print(\"\\nSample articles:\")\n",
    "                    for article in filtered_articles[:3]:\n",
    "                        print(f\"- {article['publishedAt']}: {article['title']}\")\n",
    "                \n",
    "                return {'articles': filtered_articles, 'totalResults': len(filtered_articles)}\n",
    "            \n",
    "        print(\"\\nNo articles found in date range\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to retrieve sentiment data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test with realistic dates (last 7 days)\n",
    "test_week_start = datetime.now() - timedelta(days=7)\n",
    "test_week_end = datetime.now()\n",
    "print(\"\\nTesting last week:\")\n",
    "weekly_data = test_sentiment_dates('MSFT', test_week_start, test_week_end)\n",
    "\n",
    "# Test with last month (Limited to last 30 days by NewsAPI)\n",
    "test_month_start = datetime.now() - timedelta(days=30)\n",
    "test_month_end = datetime.now()\n",
    "print(\"\\nTesting last month:\")\n",
    "monthly_data = test_sentiment_dates('MSFT', test_month_start, test_month_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e1208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified weekly sentiment calculation\n",
    "weekly_sentiments = []\n",
    "for date in tqdm(pd.date_range(start_date, end_date, freq='W'), desc=\"Processing MSFT\"):\n",
    "    try:\n",
    "        # Get sentiment for the week\n",
    "        week_start = date - timedelta(days=7)\n",
    "        features = sentiment_analyzer.get_sentiment_features('MSFT', date, week_start)\n",
    "        \n",
    "        # Add date range to debug output\n",
    "        print(f\"\\nWeek {week_start.strftime('%Y-%m-%d')} to {date.strftime('%Y-%m-%d')}:\")\n",
    "        if 'articles' in features:\n",
    "            print(f\"Articles found: {len(features['articles'])}\")\n",
    "            for article in features['articles'][:3]:\n",
    "                print(f\"- {article['publishedAt']}: {article['title']}\")\n",
    "        \n",
    "        weekly_sentiments.append({\n",
    "            'sentiment_score': features['sentiment_score'],\n",
    "            'sentiment_magnitude': features['sentiment_magnitude'],\n",
    "            'article_count': features['article_count']\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error for week of {date}: {str(e)}\")\n",
    "        logger.error(f\"Full traceback: {traceback.format_exc()}\")\n",
    "        weekly_sentiments.append({\n",
    "            'sentiment_score': None,\n",
    "            'sentiment_magnitude': None,\n",
    "            'article_count': None\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8bbf7",
   "metadata": {},
   "source": [
    "## Visualize sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b93984",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot time series\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "# Plot 1: Sentiment scores over time\n",
    "for symbol in [target_symbol] + peer_symbols:\n",
    "    axes[0].plot(sentiment_data.index, \n",
    "                 sentiment_data[f'{symbol}_sentiment_score'],\n",
    "                 label=symbol, alpha=0.7)\n",
    "axes[0].set_title('Sentiment Scores Over Time')\n",
    "axes[0].set_ylabel('Sentiment Score')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Sentiment magnitude over time\n",
    "for symbol in [target_symbol] + peer_symbols:\n",
    "    axes[1].plot(sentiment_data.index, \n",
    "                 sentiment_data[f'{symbol}_sentiment_magnitude'],\n",
    "                 label=symbol, alpha=0.7)\n",
    "axes[1].set_title('Sentiment Magnitude Over Time')\n",
    "axes[1].set_ylabel('Magnitude')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot 3: Article count over time\n",
    "for symbol in [target_symbol] + peer_symbols:\n",
    "    axes[2].plot(sentiment_data.index, \n",
    "                 sentiment_data[f'{symbol}_article_count'],\n",
    "                 label=symbol, alpha=0.7)\n",
    "axes[2].set_title('Article Count Over Time')\n",
    "axes[2].set_ylabel('Number of Articles')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save sentiment time series data\n",
    "sentiment_data.to_pickle('../data/processed/sentiment_timeseries.pkl')\n",
    "print(\"\\nSentiment time series data saved to '../data/processed/sentiment_timeseries.pkl'\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSentiment Statistics:\")\n",
    "print(sentiment_data.describe().round(3))\n",
    "\n",
    "# Calculate rolling correlations with technical indicators\n",
    "window = 7  # 7-day rolling window\n",
    "rolling_correlations = pd.DataFrame()\n",
    "\n",
    "for symbol in [target_symbol] + peer_symbols:\n",
    "    sentiment_series = sentiment_data[f'{symbol}_sentiment_score']\n",
    "    tech_indicators = all_features[[col for col in all_features.columns if symbol in col and any(\n",
    "        indicator in col for indicator in ['RSI', 'MACD', 'Signal']\n",
    "    )]]\n",
    "    \n",
    "    for col in tech_indicators.columns:\n",
    "        rolling_corr = sentiment_series.rolling(window).corr(tech_indicators[col])\n",
    "        rolling_correlations[f'{symbol}_{col}_correlation'] = rolling_corr\n",
    "\n",
    "print(\"\\nRolling Correlations Summary:\")\n",
    "print(rolling_correlations.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ebd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372794a",
   "metadata": {},
   "source": [
    "## 2. Technical Indicators (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Technical Indicators\n",
    "\n",
    "# First, let's print the available columns\n",
    "print(\"Available columns:\")\n",
    "print([col for col in all_features.columns if 'RSI' in col])\n",
    "\n",
    "# Then update the plotting code with the correct column name\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(all_features.index, all_features[f'{target_symbol}_RSI'])  # Removed _14 suffix\n",
    "plt.axhline(y=70, color='r', linestyle='--', label='Overbought (70)')\n",
    "plt.axhline(y=30, color='g', linestyle='--', label='Oversold (30)')\n",
    "plt.title(f'RSI Over Time - {target_symbol}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot MACD for target asset\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(all_features.index, all_features[f'{target_symbol}_MACD'], label='MACD')  # Simplified column names\n",
    "plt.plot(all_features.index, all_features[f'{target_symbol}_Signal'], label='Signal')\n",
    "plt.bar(all_features.index, all_features[f'{target_symbol}_Histogram'], label='Histogram')\n",
    "plt.title(f'MACD Analysis - {target_symbol}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a44b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MACD across peers\n",
    "plt.figure(figsize=(15, 6))\n",
    "for symbol in [target_symbol] + peer_symbols:\n",
    "    plt.plot(all_features.index, all_features[f'{symbol}_MACD'], label=f'{symbol}_MACD')\n",
    "    plt.plot(all_features.index, all_features[f'{symbol}_Signal'], label=f'{symbol}_Signal')\n",
    "    plt.bar(all_features.index, all_features[f'{symbol}_Histogram'], label=f'{symbol}_Histogram', alpha=0.3)\n",
    "plt.title('MACD Analysis Comparison')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d510e14d",
   "metadata": {},
   "source": [
    "## 3. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa66da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = all_features.corr()\n",
    "\n",
    "# Create a larger figure for better readability\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "# Plot correlation heatmap\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            fmt='.2f',\n",
    "            square=True,\n",
    "            linewidths=0.5)\n",
    "\n",
    "plt.title('Feature Correlation Matrix Across All Assets')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display summary of highly correlated features (|correlation| > 0.7)\n",
    "high_corr = np.where(np.abs(correlation_matrix) > 0.7)\n",
    "high_corr = [(correlation_matrix.index[x], correlation_matrix.columns[y], correlation_matrix.iloc[x, y]) \n",
    "             for x, y in zip(*high_corr) if x != y]\n",
    "\n",
    "print(\"\\nHighly correlated features (|correlation| > 0.7):\")\n",
    "for feat1, feat2, corr in sorted(high_corr, key=lambda x: abs(x[2]), reverse=True):\n",
    "    print(f\"{feat1} <-> {feat2}: {corr:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
